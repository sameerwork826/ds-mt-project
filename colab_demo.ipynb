{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5651ef",
   "metadata": {},
   "source": [
    "# English → Hindi MT — Colab demo\n",
    "This notebook demonstrates loading a pretrained EN→HI model, running quick inference, and a tiny fine-tune demo. It is Colab-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run in Colab once)\n",
    "!pip install -q transformers datasets evaluate sentencepiece sacremoses accelerate torch --quiet\n",
    "!pip install -q sacrebleu streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41805c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "translator = pipeline(\"translation_en_to_hi\", model=model, tokenizer=tokenizer, device=0 if __import__('torch').cuda.is_available() else -1)\n",
    "print(\"Loaded model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference demo\n",
    "sentences = [\"How are you?\", \"I love machine learning.\", \"Where is the nearest station?\"]\n",
    "res = translator(sentences, max_length=120, num_beams=4)\n",
    "for s, r in zip(sentences, res):\n",
    "    print(\"EN:\", s)\n",
    "    print(\"HI:\", r['translation_text'])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny fine-tune demo (toy) - this will NOT produce a useful model but shows steps\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "data = {'src': ['Hello', 'Good night'], 'tgt': ['नमस्ते', 'शुभ रात्रि']}\n",
    "ds = Dataset.from_pandas(pd.DataFrame(data))\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch['src'], truncation=True, padding='max_length', max_length=32)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch['tgt'], truncation=True, padding='max_length', max_length=32)\n",
    "    labels['input_ids'] = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']]\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "ds = ds.map(preprocess, batched=True, remove_columns=['src', 'tgt'])\n",
    "ds = ds.train_test_split(test_size=0.5)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and model to disk (useful for Streamlit demo)\n",
    "model.save_pretrained('/content/mt-en-hi-demo')\n",
    "tokenizer.save_pretrained('/content/mt-en-hi-demo')\n",
    "print('Saved to /content/mt-en-hi-demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d61ab",
   "metadata": {},
   "source": [
    "## Streamlit\n",
    "You can download the saved model folder and use it with the Streamlit demo included in the repo (`app.py` or `app_improved.py`). In Colab you can also run Streamlit via `ngrok` or `localtunnel`, but that's outside the scope of this quick demo."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
